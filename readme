Hereâ€™s the same content properly formatted in **Markdown** (`README.md`) syntax:

````markdown
# EpiCRISPROff: Training, Testing, Evaluation, and Interpretation

## ğŸš€ Quick Start

To train/test/evaluate an ensemble or a single model:

```bash
git clone <repo-url>
cd EpiCRISPROff
python main.py --argfile "Args.txt"
````

---

## ğŸ“„ `Args.txt` Configuration File

This file can be altered by the user to train/test/evaluate different model architectures and inputs.

### Arguments:

* `--model (int)`
  Values: `1-6`
  Model types. `6` is the GRU-Embedding model.

* `--cross_val (int)`
  Values:
  `2` â€“ k-cross validation
  `3` â€“ Ensemble

* `--features_method (int)`
  Values:
  `1` â€“ Only sequence
  `2` â€“ With features by columns

* `--features_columns (str)`
  Path to a JSON file with keys as feature descriptions and values as lists of features.

* `--job (str)`
  Options: `train`, `test`, `evaluation`

* `--exclude_guides (list)`
  Format: `[path, str]`
  Exclude sgRNAs from training.

  * `path`: CSV file containing guide sequences
  * `str`: Column name of the guides to exclude

* `--test_on_other_data (list)`
  Format: `[path, str]`

  * `path`: JSON file mapping data names to paths
  * `str`: Name of dataset to test on (must match a key in the JSON)

ğŸ“ *Examples can be found in the `Args_examples` folder.*

---

## ğŸ§  Models

### ğŸ”§ Training

Trains models on the 78 GUIDE-seq experiments from the CHANGE-seq study.

If `--exclude_guides` is given, it will exclude these guides and their OTSs from the training data.
Use: `--job train`

#### âœ… Ensemble Training

* Defaults: 10 ensembles Ã— 50 models
* Set `--cross_val` to `3`
* Excludes guides via `--exclude_guides`
* Training on the 78 GUIDE-seq experiments
* To change default behavior, modify `"Vivo-silico"` path in `Jsons/Data_columns_and_paths.json`

#### âœ… K-Fold Training

* Set `--cross_val` to `2`
* Do **not** provide `--exclude_guides` or `--test_on_other_data`
* Uses 10 `train_guides.txt` files from `Data_sets/train_guides`

#### ğŸ“¦ Trained Model Paths

* **K-Cross Example:**
  `Models/GRU-EMB/K_cross/Only_sequence/(k').keras`

* **Ensemble Example:**
  `Models/Exclude_Refined_TrueOT/GRU-EMB/Ensemble/With_features_by_columns/10_ensembels/50_models/Binary_epigenetics/H3K27me3/ensemble_(n)/model_(m).keras`

> âš ï¸ ENSEMBLE:
> 8 individual features + 2 combinations + only sequence = 11 models
> 11 Ã— 10 ensembles Ã— 50 models = **5500 models**
> ğŸ§  Training takes time and \~14GB storage.

---

### ğŸ§ª Testing

Use: `--job test`

Saves predictions, labels, and OTS indices.

#### âœ… Ensemble Testing

* `--test_on_other_data` **must** be provided
* If not given, ensemble is evaluated on training data
* Saves: `ensemble_(m).pkl` containing average predictions
  (Originally saved all 50 model predictions, now only average due to storage)

#### âœ… K-Cross Testing

* Set `--cross_val` to `2`
* Do **not** provide `--exclude_guides` or `--test_on_other_data`
* Uses 10 `test_guides.txt` from `Data_sets/test_guides`

#### ğŸ“‚ Test Output Paths

* **K-Cross Example:**
  `ML_results/GRU-EMB/K_cross/With_features_by_columns/Feature_name/raw_scores.pkl`

* **Ensemble Example:**
  `ML_results/Exclude_Refined_TrueOT/on_Refined_TrueOT_shapiro_park/GRU-EMB/Ensemble/With_features_by_columns/10_ensembels/50_models/Binary_epigenetics/H3K27me3/Scores/ensemble_m.pkl`

---

### ğŸ“ˆ Evaluation

Evaluates AUROC, AUPRC, and other metrics.
Use: `--job evaluation`

#### âœ… Ensemble Evaluation

* Set `--cross_val` to `3`
* Evaluates each ensemble `.pkl` score file
* Saves:

  * `all_features.pkl`: Dictionary with feature â†’ \[ensemble\_n, metric values]
  * `mean_std.pkl`, `mean_std.csv`
  * `p_val.pkl`, `p_val.csv` (used in `Figures/ROC_PR_figs.py`)

#### âœ… K-Cross Evaluation

* Set `--cross_val` to `2`
* Saves:

  * `results_summary.xlsx`: Partition-wise metrics (in `Plots/GRU-EMB/K_cross/All_partitions`)
  * `averaged_results.csv`, `p_vals.csv`: Averaged metrics + significance test
  * AUROC/AUPRC plots (in `Figures/`)

> ğŸ“Œ *P-values from Wilcoxon rank-sum test comparing "only sequence" to features*

---

## ğŸ§¬ Sequence Models

### ğŸ”¹ Only Sequence Model

* Set `features_method = 1` in `Args.txt`

### ğŸ”¹ Sequence + Features

* Set `features_method = 2`
* Set `--features_columns` to a valid path, e.g.:

  * `Jsons/feature_columns_dict_change_seq.json`
  * For HSPC testing: `Jsons/feature_columns_dict_hspc.json`

> âš ï¸ Make sure feature names in the JSON match column names in your dataset.

---

## ğŸ§  Interpretation

To interpret the **All-epigenetic model** trained on 72 GUIDE-seq experiments:

```bash
python interpertation.py
```

* Evaluates the **first ensemble** trained on all epigenetic features

* Saves SHAP object to:
  `Plots/Interpertability/SHAP_values/all_guides.pkl`

* To interpret a different ensemble:

  * Change the path in `interpertation.py`
  * Modify `run_shap()` feature list to match the trained features

You can run:

```bash
python Figures/EpiCRISPROff_interpertability_plot.py
```

To create a **beeswarm plot** of epigenetic feature importances.

```


